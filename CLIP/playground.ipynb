{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders import get_dataloader\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 335M/335M [01:14<00:00, 4.68MiB/s]\n"
     ]
    }
   ],
   "source": [
    "model, preprocess = clip.load('ViT-B/16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset: GEOLOCATION_KAGGLE.\n",
      " Transformation test: Compose(\n",
      "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=warn)\n",
      "    CenterCrop(size=(224, 224))\n",
      "    <function _convert_image_to_rgb at 0x7f92de6b5240>\n",
      "    ToTensor()\n",
      "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
      ")\n",
      " Transformation train: None\n",
      " Dataloader type: test.\n",
      " Test images 49997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/azfarm/siddhant/Geolocalization_UCF/ VLM-GeoBench/CLIP/dataloaders/GeoLocation_Kaggle.py:40: UserWarning: size mismatch is there\n",
      "  warnings.warn('size mismatch is there')\n"
     ]
    }
   ],
   "source": [
    "test = get_dataloader(\"geolocation_kaggle\", preprocess, loader_type=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(test, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch1 = next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model parameters: 149,620,737\n"
     ]
    }
   ],
   "source": [
    "print(f\" Model parameters: {np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file_location):\n",
    "    with open(file_location, 'r') as file:\n",
    "        content = file.read(); content = str(content); content = content.split('\\n', -1)\n",
    "    try: content.remove(\"\")\n",
    "    except: pass\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classes_prompts():\n",
    "    classes = read_txt(\"/data/azfarm/siddhant/Geolocalization_UCF/ VLM-GeoBench/CLIP/dataloaders/classes/GeoLocation_kaggle.txt\")\n",
    "    templates = read_txt(\"/data/azfarm/siddhant/Geolocalization_UCF/ VLM-GeoBench/CLIP/dataloaders/templates/GeoLocation_Kaggle.txt\")\n",
    "    return classes, templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, templates = get_classes_prompts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_classifier(classnames, templates, model):\n",
    "    \"\"\" \n",
    "    Creating zero-shot classifier weights. This is taken form CLIP official codebase.\n",
    "    Please refer to .\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        zeroshot_weights = []\n",
    "        for classname in tqdm(classnames):\n",
    "            texts = [template.format(classname) for template in templates] # format with class\n",
    "            texts = clip.tokenize(texts).cuda() # tokenize\n",
    "            class_embeddings = model.encode_text(texts) # embed with text encoder\n",
    "            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)\n",
    "            class_embedding = class_embeddings.mean(dim=0)\n",
    "            class_embedding /= class_embedding.norm()\n",
    "            zeroshot_weights.append(class_embedding)\n",
    "        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).cuda()\n",
    "    return zeroshot_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124/124 [00:03<00:00, 36.00it/s]\n"
     ]
    }
   ],
   "source": [
    "zeroshot_weights = zeroshot_classifier(classes, templates, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"\n",
    "    Zero-shot prediction. This is taken form CLIP official codebase.\n",
    "    Please refer to .\n",
    "    \"\"\"\n",
    "    pred = output.topk(max(topk), 1, True, True)[1].t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/782 [00:00<?, ?it/s]/tmp/ipykernel_357514/3265073315.py:8: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk]\n",
      "  3%|▎         | 20/782 [00:38<24:38,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    top1, top5, n = 0., 0., 0.\n",
    "    for i, (images, target) in enumerate(tqdm(loader)):\n",
    "        images = images.cuda()\n",
    "        target = target.cuda()\n",
    "        \n",
    "        # predict\n",
    "        image_features = model.encode_image(images)\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        logits = 100. * image_features @ zeroshot_weights\n",
    "\n",
    "        # measure accuracy\n",
    "        acc1, _ = accuracy(logits, target, topk=(1, 5))\n",
    "        top1 += acc1\n",
    "        n += images.size(0)\n",
    "        if(i == 20):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
