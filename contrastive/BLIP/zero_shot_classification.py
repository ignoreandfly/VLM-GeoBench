'''
 * Copyright (c) 2022, salesforce.com, inc.
 * All rights reserved.
 * SPDX-License-Identifier: BSD-3-Clause
 * For full license text, see LICENSE.txt file in the repo root or https://opensource.org/licenses/BSD-3-Clause
 * By Junnan Li
'''

import argparse
import os
import yaml
import numpy as np
import random
import time
import datetime
import json
from pathlib import Path

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.backends.cudnn as cudnn
import torch.distributed as dist
from torch.utils.data import DataLoader

from models.blip_retrieval import blip_retrieval
import utils
from utils import cosine_lr_schedule
from data import create_dataset, create_sampler, create_loader, create_dataset_zero_shot
from logger import setup_logger
from tqdm import tqdm
def zeroshot_classifier(classnames, templates, model, device):
    """ 
    Creating zero-shot classifier weights. This is taken form CLIP official codebase.
    Please refer to .
    """
    with torch.no_grad():
        zeroshot_weights = []
        for classname in tqdm(classnames):
            texts = [template.format(classname) for template in templates] # format with class
            text_input = model.tokenizer(texts, padding='max_length', truncation=True, max_length=35, return_tensors="pt").to(device) 
            text_output = model.text_encoder(text_input.input_ids, attention_mask = text_input.attention_mask, mode='text')  
            class_embeddings = model.text_proj(text_output.last_hidden_state[:,0,:])
            class_embeddings /= class_embeddings.norm(dim=-1, keepdim=True)
            class_embedding = class_embeddings.mean(dim=0)
            class_embedding /= class_embedding.norm()
            zeroshot_weights.append(class_embedding)
        zeroshot_weights = torch.stack(zeroshot_weights, dim=1).to(device)
    return zeroshot_weights

def accuracy(output, target, topk=(1,)):
    """
    Zero-shot prediction. This is taken form CLIP official codebase.
    Please refer to .
    """
    pred = output.topk(max(topk), 1, True, True)[1].t()
    correct = pred.eq(target.view(1, -1).expand_as(pred))
    return [float(correct[:k].reshape(-1).float().sum(0, keepdim=True).cpu().numpy()) for k in topk]

def read_txt(file_location):
    with open(file_location, 'r') as file:
        content = file.read(); content = str(content); content = content.split('\n', -1)
    try: content.remove("")
    except: pass
    return content

def extend_user_path(path, default_behavior = False):
    """
    Extend the user path to the home directory.
    """
    if default_behavior == True:
        home_directory = os.path.expanduser('~')
        if path.startswith('~'):
            return path.replace("~", home_directory)
        return path
    else:
        return path.replace("~", os.path.join("/data", "azfarm", "siddhant", "Geolocalization_UCF"))

def get_classes_prompts(args):
    args.class_dir = extend_user_path(args.class_dir)
    args.templates_dir = extend_user_path(args.templates_dir)
    classes = read_txt(os.path.join(args.class_dir, f"{args.dataset}.txt"))
    templates = read_txt(os.path.join(args.templates_dir, f"{args.dataset}.txt"))
    return classes, templates

@torch.no_grad()
def evaluation(model, data_loader, device, config):
    model.eval()
    metric_logger = utils.MetricLogger(delimiter="  ")
    header = 'Evaluation:'
    print('Computing features for evaluation...')
    
    # Creating zero-shot classifier weights
    classes, templates = get_classes_prompts(args)
    zeroshot_weights = zeroshot_classifier(classes, templates, model, device)
    
    # Extract the image features and image embedding
    image_feats = []
    image_embeds = []

    with torch.no_grad():
        top1, top5, n = 0., 0., 0.
        for i, (images, target) in enumerate(tqdm(data_loader)):
            images = images.to(device)
            image_feat = model.visual_encoder(images)
            image_embed = model.vision_proj(image_feat[:,0,:])

            target = target.cuda()
            
            # prediction
            image_embed /= image_embed.norm(dim=-1, keepdim=True)
            logits = 100. * image_embed @ zeroshot_weights

            # measure accuracy
            acc1, acc5 = accuracy(logits, target, topk=(1, 5))
            top1 += acc1
            top5 += acc5
            n += images.size(0)

    top1 = (top1 / n) * 100
    top5 = (top5 / n) * 100 

    print(f"Top-1 accuracy: {top1:.2f}")
    print(f"Top-5 accuracy: {top5:.2f}")


def compute_flops(model, verbose=False, print_per_layer_stat=False, resolution =(3, 384, 384) ):
    from ptflops import get_model_complexity_info
    import re
    macs, params = get_model_complexity_info(model.visual_encoder.float(),  resolution , as_strings=True, print_per_layer_stat=print_per_layer_stat, verbose=verbose)
    
    flops = eval(re.findall(r'([\d.]+)', macs)[0])*2
    flops_unit = re.findall(r'([A-Za-z]+)', macs)[0][0]

    print('Computational complexity: {:<8}'.format(macs))
    print('Computational complexity: {} {}Flops'.format(flops, flops_unit))
    print('Number of parameters: {:<8}'.format(params))    
    quit()


def main(args, config):
    utils.init_distributed_mode(args)    
    device = torch.device(args.device)

    # fix the seeds for reproducibility
    seed = args.seed + utils.get_rank()
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    cudnn.benchmark = True

    #### Dataset #### 
    print(f"Creating dataset {config['dataset']}")
    # Here we get the tranformation functions
    test_dataset = create_dataset_zero_shot(config['dataset'], config, low_resolution=args.resolution, root=args.dataset_dir)    
    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, num_workers=4)
    print("Test dataset object:", test_dataset)
    print("Type:", type(test_dataset))

    #### Model #### 
    print("Creating model")
    model = blip_retrieval(pretrained=config['pretrained'], image_size=config['image_size'], vit=config['vit'], 
                             vit_grad_ckpt=config['vit_grad_ckpt'], vit_ckpt_layer=config['vit_ckpt_layer'], 
                             queue_size=config['queue_size'], negative_all_rank=config['negative_all_rank'])

    # compute_flops(model)
    model = model.to(device)
    
    model_without_ddp = model
    if args.distributed:
        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])
        model_without_ddp = model.module

    evaluation(model_without_ddp, test_loader, device, config)
    
def struct_output(args):
    """ create the output folder structure ."""
    # create `output' folder
    output_folder = os.path.join(args.output_dir, "output")
    if not os.path.exists(output_folder):
        os.mkdir(output_folder)
    
    # create the dataset name as the subfolde under `output' folder  
    sub_output_folder = os.path.join(output_folder, args.dataset)
    if not os.path.exists(sub_output_folder):
        os.mkdir(sub_output_folder)
        
    # create `evaluation' type subfolder under `sub_output_folder'
    sub_output_folder = os.path.join(sub_output_folder, "zero-shot")
    if not os.path.exists(sub_output_folder):
        os.mkdir(sub_output_folder)

    # create the logger
    name_of_file = f"log_{os.path.basename(args.config)}_{args.resolution}"
    name_of_file = name_of_file.replace("/", "")
    setup_logger(output=sub_output_folder, name_of_file=name_of_file)
    
if __name__ == '__main__':
    parser = argparse.ArgumentParser()     
    parser.add_argument('--config', default='pretrained_ViTB14.yaml', type=str, help='config file for the model')
    parser.add_argument('--output_dir', default='./output/')        
    parser.add_argument('--evaluate', action='store_true')
    parser.add_argument('--device', default='cuda')
    parser.add_argument('--seed', default=42, type=int)
    parser.add_argument('--world_size', default=1, type=int, help='number of distributed processes')    
    parser.add_argument('--dist_url', default='env://', help='url used to set up distributed training')
    parser.add_argument('--distributed', default=True, type=bool)
    parser.add_argument('--resolution', default=224, type=int, help="low resolution size")
    parser.add_argument("--dataset",
        type=str,
        default="geolocation_kaggle",
        help="Dataset name (small leter recombedded)",
        choices=['geolocation_kaggle'],
    )
    parser.add_argument('--batch_size',
        default=256,
        type=int,
        help="test batch size"
    )
    parser.add_argument("--dtd_split",
        type=int,
        default=1,
        help="Split number for DTD dataset, for other dataset you can ignore this.",
    )
    parser.add_argument("--class_dir",
        type=str,
        default="~/VLM-GeoBench/CLIP/dataloaders/classes/",
        help="input image resolution for model",
    )
    parser.add_argument("--templates_dir",
        type=str,
        default="~/VLM-GeoBench/CLIP/dataloaders/templates",
        help="input image resolution for model",
    )
    parser.add_argument("--dataset_dir", type=str, default=None, help="input image resolution for model",)
    

    args = parser.parse_args()
    current_dir = os.path.dirname(os.path.abspath(__file__))
    args.config = os.path.join(current_dir, "configs", args.config)

    config = yaml.load(open(args.config, 'r'), Loader=yaml.Loader)
    Path(args.output_dir).mkdir(parents=True, exist_ok=True)

    config['batch_size_test'] = args.batch_size
    config['dataset'] = args.dataset
    config['image_root'] = ""



    struct_output(args)
    main(args, config)